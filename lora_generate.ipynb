{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6\"\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer,GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\",\n",
    "                                         load_in_8bit=True,\n",
    "                                         device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, \t cuda:0 \ttorch.float16\n",
      "1, \t cuda:0 \ttorch.int8\n",
      "2, \t cuda:0 \ttorch.int8\n",
      "3, \t cuda:0 \ttorch.int8\n",
      "4, \t cuda:0 \ttorch.int8\n",
      "5, \t cuda:0 \ttorch.int8\n",
      "6, \t cuda:0 \ttorch.int8\n",
      "7, \t cuda:0 \ttorch.int8\n",
      "8, \t cuda:0 \ttorch.float16\n",
      "9, \t cuda:0 \ttorch.float16\n",
      "10, \t cuda:0 \ttorch.int8\n",
      "11, \t cuda:0 \ttorch.int8\n",
      "12, \t cuda:0 \ttorch.int8\n",
      "13, \t cuda:0 \ttorch.int8\n",
      "14, \t cuda:0 \ttorch.int8\n",
      "15, \t cuda:0 \ttorch.int8\n",
      "16, \t cuda:0 \ttorch.int8\n",
      "17, \t cuda:0 \ttorch.float16\n",
      "18, \t cuda:0 \ttorch.float16\n",
      "19, \t cuda:0 \ttorch.int8\n",
      "20, \t cuda:0 \ttorch.int8\n",
      "21, \t cuda:0 \ttorch.int8\n",
      "22, \t cuda:0 \ttorch.int8\n",
      "23, \t cuda:0 \ttorch.int8\n",
      "24, \t cuda:0 \ttorch.int8\n",
      "25, \t cuda:0 \ttorch.int8\n",
      "26, \t cuda:0 \ttorch.float16\n",
      "27, \t cuda:0 \ttorch.float16\n",
      "28, \t cuda:0 \ttorch.int8\n",
      "29, \t cuda:0 \ttorch.int8\n",
      "30, \t cuda:0 \ttorch.int8\n",
      "31, \t cuda:0 \ttorch.int8\n",
      "32, \t cuda:0 \ttorch.int8\n",
      "33, \t cuda:0 \ttorch.int8\n",
      "34, \t cuda:0 \ttorch.int8\n",
      "35, \t cuda:0 \ttorch.float16\n",
      "36, \t cuda:0 \ttorch.float16\n",
      "37, \t cuda:1 \ttorch.int8\n",
      "38, \t cuda:1 \ttorch.int8\n",
      "39, \t cuda:1 \ttorch.int8\n",
      "40, \t cuda:1 \ttorch.int8\n",
      "41, \t cuda:1 \ttorch.int8\n",
      "42, \t cuda:1 \ttorch.int8\n",
      "43, \t cuda:1 \ttorch.int8\n",
      "44, \t cuda:1 \ttorch.float16\n",
      "45, \t cuda:1 \ttorch.float16\n",
      "46, \t cuda:1 \ttorch.int8\n",
      "47, \t cuda:1 \ttorch.int8\n",
      "48, \t cuda:1 \ttorch.int8\n",
      "49, \t cuda:1 \ttorch.int8\n",
      "50, \t cuda:1 \ttorch.int8\n",
      "51, \t cuda:1 \ttorch.int8\n",
      "52, \t cuda:1 \ttorch.int8\n",
      "53, \t cuda:1 \ttorch.float16\n",
      "54, \t cuda:1 \ttorch.float16\n",
      "55, \t cuda:1 \ttorch.int8\n",
      "56, \t cuda:1 \ttorch.int8\n",
      "57, \t cuda:1 \ttorch.int8\n",
      "58, \t cuda:1 \ttorch.int8\n",
      "59, \t cuda:1 \ttorch.int8\n",
      "60, \t cuda:1 \ttorch.int8\n",
      "61, \t cuda:1 \ttorch.int8\n",
      "62, \t cuda:1 \ttorch.float16\n",
      "63, \t cuda:1 \ttorch.float16\n",
      "64, \t cuda:1 \ttorch.int8\n",
      "65, \t cuda:1 \ttorch.int8\n",
      "66, \t cuda:1 \ttorch.int8\n",
      "67, \t cuda:1 \ttorch.int8\n",
      "68, \t cuda:1 \ttorch.int8\n",
      "69, \t cuda:1 \ttorch.int8\n",
      "70, \t cuda:1 \ttorch.int8\n",
      "71, \t cuda:1 \ttorch.float16\n",
      "72, \t cuda:1 \ttorch.float16\n",
      "73, \t cuda:1 \ttorch.int8\n",
      "74, \t cuda:1 \ttorch.int8\n",
      "75, \t cuda:1 \ttorch.int8\n",
      "76, \t cuda:1 \ttorch.int8\n",
      "77, \t cuda:1 \ttorch.int8\n",
      "78, \t cuda:1 \ttorch.int8\n",
      "79, \t cuda:1 \ttorch.int8\n",
      "80, \t cuda:1 \ttorch.float16\n",
      "81, \t cuda:1 \ttorch.float16\n",
      "82, \t cuda:1 \ttorch.int8\n",
      "83, \t cuda:1 \ttorch.int8\n",
      "84, \t cuda:1 \ttorch.int8\n",
      "85, \t cuda:1 \ttorch.int8\n",
      "86, \t cuda:1 \ttorch.int8\n",
      "87, \t cuda:1 \ttorch.int8\n",
      "88, \t cuda:1 \ttorch.int8\n",
      "89, \t cuda:1 \ttorch.float16\n",
      "90, \t cuda:1 \ttorch.float16\n",
      "91, \t cuda:2 \ttorch.int8\n",
      "92, \t cuda:2 \ttorch.int8\n",
      "93, \t cuda:2 \ttorch.int8\n",
      "94, \t cuda:2 \ttorch.int8\n",
      "95, \t cuda:2 \ttorch.int8\n",
      "96, \t cuda:2 \ttorch.int8\n",
      "97, \t cuda:2 \ttorch.int8\n",
      "98, \t cuda:2 \ttorch.float16\n",
      "99, \t cuda:2 \ttorch.float16\n",
      "100, \t cuda:2 \ttorch.int8\n",
      "101, \t cuda:2 \ttorch.int8\n",
      "102, \t cuda:2 \ttorch.int8\n",
      "103, \t cuda:2 \ttorch.int8\n",
      "104, \t cuda:2 \ttorch.int8\n",
      "105, \t cuda:2 \ttorch.int8\n",
      "106, \t cuda:2 \ttorch.int8\n",
      "107, \t cuda:2 \ttorch.float16\n",
      "108, \t cuda:2 \ttorch.float16\n",
      "109, \t cuda:2 \ttorch.int8\n",
      "110, \t cuda:2 \ttorch.int8\n",
      "111, \t cuda:2 \ttorch.int8\n",
      "112, \t cuda:2 \ttorch.int8\n",
      "113, \t cuda:2 \ttorch.int8\n",
      "114, \t cuda:2 \ttorch.int8\n",
      "115, \t cuda:2 \ttorch.int8\n",
      "116, \t cuda:2 \ttorch.float16\n",
      "117, \t cuda:2 \ttorch.float16\n",
      "118, \t cuda:2 \ttorch.int8\n",
      "119, \t cuda:2 \ttorch.int8\n",
      "120, \t cuda:2 \ttorch.int8\n",
      "121, \t cuda:2 \ttorch.int8\n",
      "122, \t cuda:2 \ttorch.int8\n",
      "123, \t cuda:2 \ttorch.int8\n",
      "124, \t cuda:2 \ttorch.int8\n",
      "125, \t cuda:2 \ttorch.float16\n",
      "126, \t cuda:2 \ttorch.float16\n",
      "127, \t cuda:2 \ttorch.int8\n",
      "128, \t cuda:2 \ttorch.int8\n",
      "129, \t cuda:2 \ttorch.int8\n",
      "130, \t cuda:2 \ttorch.int8\n",
      "131, \t cuda:2 \ttorch.int8\n",
      "132, \t cuda:2 \ttorch.int8\n",
      "133, \t cuda:2 \ttorch.int8\n",
      "134, \t cuda:2 \ttorch.float16\n",
      "135, \t cuda:2 \ttorch.float16\n",
      "136, \t cuda:2 \ttorch.int8\n",
      "137, \t cuda:2 \ttorch.int8\n",
      "138, \t cuda:2 \ttorch.int8\n",
      "139, \t cuda:2 \ttorch.int8\n",
      "140, \t cuda:2 \ttorch.int8\n",
      "141, \t cuda:2 \ttorch.int8\n",
      "142, \t cuda:2 \ttorch.int8\n",
      "143, \t cuda:2 \ttorch.float16\n",
      "144, \t cuda:2 \ttorch.float16\n",
      "145, \t cuda:3 \ttorch.int8\n",
      "146, \t cuda:3 \ttorch.int8\n",
      "147, \t cuda:3 \ttorch.int8\n",
      "148, \t cuda:3 \ttorch.int8\n",
      "149, \t cuda:3 \ttorch.int8\n",
      "150, \t cuda:3 \ttorch.int8\n",
      "151, \t cuda:3 \ttorch.int8\n",
      "152, \t cuda:3 \ttorch.float16\n",
      "153, \t cuda:3 \ttorch.float16\n",
      "154, \t cuda:3 \ttorch.int8\n",
      "155, \t cuda:3 \ttorch.int8\n",
      "156, \t cuda:3 \ttorch.int8\n",
      "157, \t cuda:3 \ttorch.int8\n",
      "158, \t cuda:3 \ttorch.int8\n",
      "159, \t cuda:3 \ttorch.int8\n",
      "160, \t cuda:3 \ttorch.int8\n",
      "161, \t cuda:3 \ttorch.float16\n",
      "162, \t cuda:3 \ttorch.float16\n",
      "163, \t cuda:3 \ttorch.int8\n",
      "164, \t cuda:3 \ttorch.int8\n",
      "165, \t cuda:3 \ttorch.int8\n",
      "166, \t cuda:3 \ttorch.int8\n",
      "167, \t cuda:3 \ttorch.int8\n",
      "168, \t cuda:3 \ttorch.int8\n",
      "169, \t cuda:3 \ttorch.int8\n",
      "170, \t cuda:3 \ttorch.float16\n",
      "171, \t cuda:3 \ttorch.float16\n",
      "172, \t cuda:3 \ttorch.int8\n",
      "173, \t cuda:3 \ttorch.int8\n",
      "174, \t cuda:3 \ttorch.int8\n",
      "175, \t cuda:3 \ttorch.int8\n",
      "176, \t cuda:3 \ttorch.int8\n",
      "177, \t cuda:3 \ttorch.int8\n",
      "178, \t cuda:3 \ttorch.int8\n",
      "179, \t cuda:3 \ttorch.float16\n",
      "180, \t cuda:3 \ttorch.float16\n",
      "181, \t cuda:3 \ttorch.int8\n",
      "182, \t cuda:3 \ttorch.int8\n",
      "183, \t cuda:3 \ttorch.int8\n",
      "184, \t cuda:3 \ttorch.int8\n",
      "185, \t cuda:3 \ttorch.int8\n",
      "186, \t cuda:3 \ttorch.int8\n",
      "187, \t cuda:3 \ttorch.int8\n",
      "188, \t cuda:3 \ttorch.float16\n",
      "189, \t cuda:3 \ttorch.float16\n",
      "190, \t cuda:3 \ttorch.int8\n",
      "191, \t cuda:3 \ttorch.int8\n",
      "192, \t cuda:3 \ttorch.int8\n",
      "193, \t cuda:3 \ttorch.int8\n",
      "194, \t cuda:3 \ttorch.int8\n",
      "195, \t cuda:3 \ttorch.int8\n",
      "196, \t cuda:3 \ttorch.int8\n",
      "197, \t cuda:3 \ttorch.float16\n",
      "198, \t cuda:3 \ttorch.float16\n",
      "199, \t cuda:4 \ttorch.int8\n",
      "200, \t cuda:4 \ttorch.int8\n",
      "201, \t cuda:4 \ttorch.int8\n",
      "202, \t cuda:4 \ttorch.int8\n",
      "203, \t cuda:4 \ttorch.int8\n",
      "204, \t cuda:4 \ttorch.int8\n",
      "205, \t cuda:4 \ttorch.int8\n",
      "206, \t cuda:4 \ttorch.float16\n",
      "207, \t cuda:4 \ttorch.float16\n",
      "208, \t cuda:4 \ttorch.int8\n",
      "209, \t cuda:4 \ttorch.int8\n",
      "210, \t cuda:4 \ttorch.int8\n",
      "211, \t cuda:4 \ttorch.int8\n",
      "212, \t cuda:4 \ttorch.int8\n",
      "213, \t cuda:4 \ttorch.int8\n",
      "214, \t cuda:4 \ttorch.int8\n",
      "215, \t cuda:4 \ttorch.float16\n",
      "216, \t cuda:4 \ttorch.float16\n",
      "217, \t cuda:4 \ttorch.int8\n",
      "218, \t cuda:4 \ttorch.int8\n",
      "219, \t cuda:4 \ttorch.int8\n",
      "220, \t cuda:4 \ttorch.int8\n",
      "221, \t cuda:4 \ttorch.int8\n",
      "222, \t cuda:4 \ttorch.int8\n",
      "223, \t cuda:4 \ttorch.int8\n",
      "224, \t cuda:4 \ttorch.float16\n",
      "225, \t cuda:4 \ttorch.float16\n",
      "226, \t cuda:4 \ttorch.int8\n",
      "227, \t cuda:4 \ttorch.int8\n",
      "228, \t cuda:4 \ttorch.int8\n",
      "229, \t cuda:4 \ttorch.int8\n",
      "230, \t cuda:4 \ttorch.int8\n",
      "231, \t cuda:4 \ttorch.int8\n",
      "232, \t cuda:4 \ttorch.int8\n",
      "233, \t cuda:4 \ttorch.float16\n",
      "234, \t cuda:4 \ttorch.float16\n",
      "235, \t cuda:4 \ttorch.int8\n",
      "236, \t cuda:4 \ttorch.int8\n",
      "237, \t cuda:4 \ttorch.int8\n",
      "238, \t cuda:4 \ttorch.int8\n",
      "239, \t cuda:4 \ttorch.int8\n",
      "240, \t cuda:4 \ttorch.int8\n",
      "241, \t cuda:4 \ttorch.int8\n",
      "242, \t cuda:4 \ttorch.float16\n",
      "243, \t cuda:4 \ttorch.float16\n",
      "244, \t cuda:4 \ttorch.int8\n",
      "245, \t cuda:4 \ttorch.int8\n",
      "246, \t cuda:4 \ttorch.int8\n",
      "247, \t cuda:4 \ttorch.int8\n",
      "248, \t cuda:4 \ttorch.int8\n",
      "249, \t cuda:4 \ttorch.int8\n",
      "250, \t cuda:4 \ttorch.int8\n",
      "251, \t cuda:4 \ttorch.float16\n",
      "252, \t cuda:4 \ttorch.float16\n",
      "253, \t cuda:5 \ttorch.int8\n",
      "254, \t cuda:5 \ttorch.int8\n",
      "255, \t cuda:5 \ttorch.int8\n",
      "256, \t cuda:5 \ttorch.int8\n",
      "257, \t cuda:5 \ttorch.int8\n",
      "258, \t cuda:5 \ttorch.int8\n",
      "259, \t cuda:5 \ttorch.int8\n",
      "260, \t cuda:5 \ttorch.float16\n",
      "261, \t cuda:5 \ttorch.float16\n",
      "262, \t cuda:5 \ttorch.int8\n",
      "263, \t cuda:5 \ttorch.int8\n",
      "264, \t cuda:5 \ttorch.int8\n",
      "265, \t cuda:5 \ttorch.int8\n",
      "266, \t cuda:5 \ttorch.int8\n",
      "267, \t cuda:5 \ttorch.int8\n",
      "268, \t cuda:5 \ttorch.int8\n",
      "269, \t cuda:5 \ttorch.float16\n",
      "270, \t cuda:5 \ttorch.float16\n",
      "271, \t cuda:5 \ttorch.int8\n",
      "272, \t cuda:5 \ttorch.int8\n",
      "273, \t cuda:5 \ttorch.int8\n",
      "274, \t cuda:5 \ttorch.int8\n",
      "275, \t cuda:5 \ttorch.int8\n",
      "276, \t cuda:5 \ttorch.int8\n",
      "277, \t cuda:5 \ttorch.int8\n",
      "278, \t cuda:5 \ttorch.float16\n",
      "279, \t cuda:5 \ttorch.float16\n",
      "280, \t cuda:5 \ttorch.int8\n",
      "281, \t cuda:5 \ttorch.int8\n",
      "282, \t cuda:5 \ttorch.int8\n",
      "283, \t cuda:5 \ttorch.int8\n",
      "284, \t cuda:5 \ttorch.int8\n",
      "285, \t cuda:5 \ttorch.int8\n",
      "286, \t cuda:5 \ttorch.int8\n",
      "287, \t cuda:5 \ttorch.float16\n",
      "288, \t cuda:5 \ttorch.float16\n",
      "289, \t cuda:5 \ttorch.float16\n",
      "290, \t cuda:5 \ttorch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, para in enumerate(model.named_parameters()):\n",
    "#     print(f'{i}, {para[0]}\\t {para[1].device} \\t{para[1].dtype}')\n",
    "    print(f'{i}, \\t {para[1].device} \\t{para[1].dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='decapoda-research/llama-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mypeft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_in_8bit True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 model = PeftModel.from_pretrained(model,<span style=\"color: #808000; text-decoration-color: #808000\">\"tloen/alpaca-lora-7b\"</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/myb/alpaca-lora/mypeft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">186</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 183 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>low_zero=(device_map == <span style=\"color: #808000; text-decoration-color: #808000\">\"balanced_low_0\"</span>),                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 184 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 185 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(device_map, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 186 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>device_map = infer_auto_device_map(                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 187 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>model, max_memory=max_memory, no_split_module_classes=no_split_modul  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 188 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 189 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/myb/anaconda3/envs/fastchat/lib/python3.8/site-packages/accelerate/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">685</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">infer_auto_device_map</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 682 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>tied_module_names = []                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 683 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>tied_modules = []                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 684 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> tied_param <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> tied_params:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 685 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tied_module_index = [i <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, (n, _) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(modules_to_treat) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> n  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tied_module_names.append(modules_to_treat[tied_module_index][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>])          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 687 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tied_modules.append(modules_to_treat[tied_module_index][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>])               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 688 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> verbose:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>list index out of range\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 model = PeftModel.from_pretrained(model,\u001b[33m\"\u001b[0m\u001b[33mtloen/alpaca-lora-7b\u001b[0m\u001b[33m\"\u001b[0m)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/myb/alpaca-lora/mypeft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m186\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 183 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlow_zero=(device_map == \u001b[33m\"\u001b[0m\u001b[33mbalanced_low_0\u001b[0m\u001b[33m\"\u001b[0m),                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 184 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 185 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(device_map, \u001b[96mstr\u001b[0m):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 186 \u001b[2m│   │   │   │   \u001b[0mdevice_map = infer_auto_device_map(                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 187 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmodel, max_memory=max_memory, no_split_module_classes=no_split_modul  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 188 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 189 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/myb/anaconda3/envs/fastchat/lib/python3.8/site-packages/accelerate/utils/\u001b[0m\u001b[1;33mmodeling.py\u001b[0m:\u001b[94m685\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92minfer_auto_device_map\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 682 \u001b[0m\u001b[2m│   │   │   \u001b[0mtied_module_names = []                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 683 \u001b[0m\u001b[2m│   │   │   \u001b[0mtied_modules = []                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 684 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m tied_param \u001b[95min\u001b[0m tied_params:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 685 \u001b[2m│   │   │   │   \u001b[0mtied_module_index = [i \u001b[94mfor\u001b[0m i, (n, _) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(modules_to_treat) \u001b[94mif\u001b[0m n  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtied_module_names.append(modules_to_treat[tied_module_index][\u001b[94m0\u001b[0m])          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 687 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtied_modules.append(modules_to_treat[tied_module_index][\u001b[94m1\u001b[0m])               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 688 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m verbose:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mlist index out of range\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model,\"tloen/alpaca-lora-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mypeft import mapping\n",
    "from mypeft.utils import other\n",
    "\n",
    "print(\"model_type\",model.config.model_type)\n",
    "# print(model.peft_config['default'].target_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1.5,\n",
    "    # nucleus sampling\n",
    "    top_p=0.8,\n",
    "    num_beams=4,\n",
    ")\n",
    "\n",
    "def inference(instruction, input=None):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "#     print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for s in generation_output.sequences:\n",
    "        print(s)\n",
    "        output = tokenizer.decode(s)\n",
    "        print(\"Response:\", output.split(\"### Response:\")[1].strip())\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(input(\"Instruction: \"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
